<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">How to start contributing to Drools Executable Model</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Mql1NTVjhu8/how-to-start-contributing-to-drools-executable-model.html" /><author><name>Luca Molteni</name></author><id>https://blog.kie.org/2021/07/how-to-start-contributing-to-drools-executable-model.html</id><updated>2021-07-01T13:45:17Z</updated><content type="html">WHAT IS THE EXECUTABLE MODEL? The Executable Model is a new way to execute business rules in Drools. It’s based on a Java representation of the rule structure that provides a few advantages such as faster startup time and better memory allocation at runtime. You can check out the details in the or in other blog posts such as . KJARs built with the kie-maven-plugin have the Executable Model enabled since 7.33.0.Final by default and it’s the main technology underneath . With the "Executable Model Compiler", a module you can find in the drools-model-compiler directory, DRL files are transformed into a Java DSL. HOW TO START CONTRIBUTING Drools is a really big open source project, and finding the best way to contribute to it might not be easy. Luckily, the Executable Model Compiler is a good way to start, for various reasons: * It’s a fairly new project (as today it’s been more or less three years since the inception) * It doesn’t require deep understanding of the Drools’ internal algorithm, PHREAK * There always is a former counterpart to verify the code against Regarding the third point, we want Drools to behave in the exact same way while using the former runtime (also called DRL) and the new one (called PatternDSL). CONTRIBUTING: SHOWING A PROBLEM Imagine that you’re interested in contributing to Drools, what should you do when you find a problem and you think it’s related to the Executable Model? Firstly we should understand where the problem is in Drools and if it’s eventually related to the Executable Model. To do that, we need to create the smallest piece of code that shows the problem: this is what we called a "bug reproducer" (also just "reproducer"). If you provide a bug report to , or the team will ask you to create such reproducer. There are two ways to do it: 1) If you’re familiar with the you can write the test directly in your own fork of the original repository and create a PR against it. This is probably the best way to proceed, as it allows all the Drools’ developers to check the problem faster 2) Create another separate project that shows the problem METHOD #1: CREATE A REPRODUCER IN DROOLS.GIT Start by building the Drools project reading the page. You can either decide to build it using or building only the module with mvn clean install. The second one is definitely faster. Once you have the project up and running you can open it with your preferred IDE and take a look at the tests in the drools-model-compiler module, for example org.drools.modelcompiler.CompilerTest. If you run these tests you’ll see they’re executed twice, once against the DRL mode and the other against the PatternDSL. It’s important that the tests run in both ways. If you see a difference in the execution, please create a PR. And if you want to fix it on your own, try – we love to see new contributors. METHOD #2: CREATE A SELF CONTAINED PROJECT Let’s use the Drools archetype and verify that your small reproducer is working against Drools Legacy. Start by creating a KJAR using the , modify the rules and the test to verify everything is working accordingly. The default archetype will run the test against the DRL mode. Change the generated model to build an executable model KJAR. To do so, switch in the pom.xml from drools-engine-classic to drools-engine. Also add the drools-model-compiler dependency. Compile the project using maven and the command line. Use mvn clean install -DskipTests=true as it’ll try run the tests using the classic engine but we don’t have the drools-mvel dependency in the class path anymore. Verify the Executable Model has been built in the KJAR, you can for example using this command to view the content inside the KJAR: jar -tf target/name_of_the_kjar.jar You will see all the Executable Model classes under and the drools-model file. Another way to do it is to check the maven log for this phrase: [INFO] Found 7 generated files in Canonical Model [INFO] Generating /Users/lmolteni/git/contribute/reproducer-kjar/target/generated-sources/drools-model-compiler/main/java/./org/example/P41/LambdaExtractor41A2683D222972683028514525A5437B.java ... Create another project called runner that has the original project as a dependency in the Maven pom.xml. The same archetype can be used again, but you have to change a few things * Remove all the classes in src/main/java * Remove the DRL files, as this project only consumes KJARs * Remove the kmodule.xml file * Remove the kie-maven-plugin from the build. Again, we don’t want to build KJARs here, only to use them. * Switch in the pom.xml from drools-engine-classic to drools-engine. * Remove the kjar packaging in pom.xml * Add the original KJAR dependency &lt;dependency&gt; &lt;groupId&gt;org.example&lt;/groupId&gt; &lt;artifactId&gt;reproducer-kjar&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; * Move the test from the original reproducer to this module * Never call kContainer.verify(); while using Executable Model KJARs as this will retrigger the build Run the test. This time the test will run using the Executable Model. You can see it because there will be this line in the logs 2021-06-15 11:32:29,576 INFO [org.drools.modelcompiler.CanonicalKieModuleProvider] (main) Artifact org.example:reproducer-kjar:1.0-SNAPSHOT has executable model This can probably be abstracted in a new archetype, let us know if you’re interested and we can work of it. SUMMARY In this article, we saw how to provide a small reproducer to verify an unexpected behaviour in Drools Executable Model and to provide the developers some way to verify the unexpected behaviour. Please try and contribute to Drools Executable Model! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Mql1NTVjhu8" height="1" width="1" alt=""/&gt;</content><dc:creator>Luca Molteni</dc:creator><feedburner:origLink>https://blog.kie.org/2021/07/how-to-start-contributing-to-drools-executable-model.html</feedburner:origLink></entry><entry><title>How to expose a WebSocket endpoint using Red Hat 3scale API Management</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/PdloXwnONXI/how-expose-websocket-endpoint-using-red-hat-3scale-api-management" /><author><name>Srikanth Valluru</name></author><id>e0eb6645-8d5f-48c5-a701-23e1da644d49</id><updated>2021-07-01T07:00:00Z</updated><published>2021-07-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.w3.org/TR/websockets/"&gt;WebSocket&lt;/a&gt; is a communications protocol that provides full-duplex communication channels to web servers and clients over a single TCP connection. The protocol was standardized by the World Wide Web Consortium (W3C) and has been in common use by web developers for more than a decade.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/3scale/overview"&gt;Red Hat 3scale API Management&lt;/a&gt; is a hosted environment for web applications. In this quick tip, you will see how to use 3scale to set up WebSocket communication easily. Figure 1 shows how 3scale mediates between the web client and the WebSocket interface on the server.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/3scale_relationships.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/3scale_relationships.png?itok=FQROWJnz" width="600" height="259" alt="The 3scale WebSockets policy stands between the client and server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The relationship between the browser, 3scale, and the server. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;This tip takes you through the following steps:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Setting up the WebSocket server.&lt;/li&gt; &lt;li&gt;Configuring 3scale API Management.&lt;/li&gt; &lt;li&gt;Using a WebSocket client to test the WebSocket endpoint.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Step 1: Set up the WebSocket server&lt;/h2&gt; &lt;p&gt;You can use any of your favorite frameworks to start the WebSocket server. For this article, we use &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;. (Installing Node.js is out of the scope of this tip.)&lt;/p&gt; &lt;p&gt;We'll also use a simple JavaScript program that sets up a WebSocket server, accepts a request, and sends a reply. You can save it as &lt;code&gt;index.js&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="javascript"&gt;// Minimal amount of secure websocket server var fs = require('fs'); // read ssl certificate var privateKey = fs.readFileSync('ssl-cert/key.pem', 'utf8'); var certificate = fs.readFileSync('ssl-cert/certificate.pem', 'utf8'); var credentials = { key: privateKey, cert: certificate }; var https = require('https'); //pass in your credentials to create an https server var httpsServer = https.createServer(credentials); httpsServer.listen(8443,"0.0.0.0"); var WebSocketServer = require('ws').Server; var wss = new WebSocketServer({ server: httpsServer }); wss.on('connection', function connection(ws) { ws.on('message', function incoming(message) { console.log('received: %s', message); ws.send('reply from server : ' + message) }); ws.send('something'); });&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can use Node.js to start the script:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;$ node index.js&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 2: Configure 3scale API Management&lt;/h2&gt; &lt;p&gt;Follow the 3scale documentation to add a back end and create the necessary metrics, products, and application plan to expose an endpoint. Provide the WebSocket server URL as the Private Base URL, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/private_base_url.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/private_base_url.png?itok=6HmnjAxN" width="600" height="116" alt="Enter the WebSocket server URL as the Private Base URL." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Enter the WebSocket server URL in the Private Base URL field. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Add your WebSockets policy to the policy chain, as shown in Figure 3. No configuration is needed inside the policy.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/policy_chain.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/policy_chain.png?itok=CtngE58j" width="479" height="241" alt="Using the 3scale dialog to define the policy chain." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Configuring the policy chain in 3scale. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Promote the endpoint to the staging API Gateway for testing. Figure 4 shows how the endpoint and mapping rules appear in the console.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/endpoint_mapping.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/endpoint_mapping.png?itok=tCNDAWdH" width="600" height="82" alt="Viewing the server's endpoint and mapping rules in the console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: View the endpoint and mapping rules in the console. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 3: Use a WebSocket client to test the WebSocket endpoint&lt;/h2&gt; &lt;p&gt;A convenient client we use for testing in this example is the Chrome browser's &lt;a href="https://chrome.google.com/webstore/detail/web-socket-client/lifhekgaodigcpmnakfhaaaboididbdn"&gt;Web Socket Client extension&lt;/a&gt;. Enter the staging API Gateway URL and append the WebSocket public path to connect, as shown in Figure 5.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/test_url.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/test_url.png?itok=LSPvsFLt" width="600" height="140" alt="Testing a 3scale WebSocket connection by entering a URL." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: A sample URL for testing a 3scale WebSocket connection. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;3scale API Management offers policies to support communication between your front end and back end. See these resources for further information:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html/installing_3scale/installing-apicast#websocket-protocol-support-for-apicast"&gt;WebSocket policy in 3scale&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html/installing_3scale/installing-apicast#websocket-protocol-support-for-apicast"&gt;WebSocket protocol support for APIcast&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.8/html/administering_the_api_gateway/apicast_policies"&gt;Supported Policies in 3scale&lt;/a&gt; &lt;ul&gt;&lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/07/01/how-expose-websocket-endpoint-using-red-hat-3scale-api-management" title="How to expose a WebSocket endpoint using Red Hat 3scale API Management"&gt;How to expose a WebSocket endpoint using Red Hat 3scale API Management&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/PdloXwnONXI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Srikanth Valluru</dc:creator><dc:date>2021-07-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/07/01/how-expose-websocket-endpoint-using-red-hat-3scale-api-management</feedburner:origLink></entry><entry><title type="html">Cloud Adoption - Example adoption architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/feWsgR4IXJI/cloud-adoption-example-adoption-architeccture.html" /><author><name>Eric D. Schabell</name></author><id>http://feedproxy.google.com/~r/schabell/jboss/~3/UQMoqclrSI0/cloud-adoption-example-adoption-architeccture.html</id><updated>2021-07-01T05:00:00Z</updated><content type="html">Part 3 - Example adoption architecture In our  from this series shared a look at the logical common architectural elements found in a cloud adoption solution for retail organisations. The process was laid out how we've approached the use case and how portfolio solutions are the base for researching a generic architecture.  It continued by laying out the process of how we've approached the use case by researching successful customer portfolio solutions as the basis for a generic architecture. Having completed our discussions on the logical view of the architecture, it's now time to look at a specific example. This article walks you through an example cloud adoption scenario showing how expanding the previously discussed elements provides an example for your own cloud adoption scenarios. ARCHITECTURE REVIEW As mentioned before, the architectural details covered here are base on real solutions using open source technologies. The example scenario presented here is a generic common architecture that was uncovered researching those solutions. It's our intent to provide guidance and not deep technical details. This section covers the visual representations as presented, but it's expected that they'll be evolving based on future research. There are many ways to represent each element in this architecture, but we've chosen a format that we hope makes it easy to absorb. Feel free to post comments at the bottom of this post, or  with your feedback. Now let's take a look at the details in this architecture and outline the solution for a cloud adoption architecture solution. CLOUD ADOPTION The key to this cloud adoption architecture is the focus on providing the ability to move workloads, be that traditional server hosted or more modern container hosted, from the traditional data center to private or pubic clouds as needed.  Keeping that in mind, this architecture starts on the far left with the generic elements showing how a core data center such as the development teams manage their production. They have their projects in a source code management (SCM) system, which makes use of a method to build out their applications and images shown as a server image build pipeline, and some form of an image store or registry for distribution across their architecture as needed. Moving to the right you encounter the destinations for these workloads, from the traditional physical data center, private cloud, to the representation of multiple possible public clouds. Closer examination of each destination shows a simplified generic RHEL host, which can be a physical, virtual, or container based machine along with the image registry used to manage the images for their particular destination as distributed by the central development image store. Next up, infrastructure management where we find the smart management element that's gathering input from all the deployed host machines from every destination and working together automation orchestration element to manage workloads. From the gained insights into your organisations workloads, it's possible to deploy new updates, manage security patching across all infrastructure destinations, roll out extra resources for surging demand on specific workloads, and so much more. The showcase model is that a workload is determined, based on organisational standards set by you, to be a candidate for migration from the physical data center to any one of the public clouds. This could be due to cost reductions that are achievable due to changes in public cloud offerings, or due to managing performance by putting a certain workload closer to the customers actual physical location. Finally, to assist with analysing the data provided by the running hosts, there are cloud services meant to help you manage responses and maintain your repository of automated actions. Over time your automation needs change such that you have a repository of actions you might want to take, which is managed by the enterprise operating automation element. These are fed to the infrastructure management elements for use across the organisation. Also over time you'll develop plans to react on certain insights as they happen and this collection of plans can be found in the insights platform that works through insight services to support the infrastructure management elements. As you can see, automating your cloud infrastructure requires insights based plans and actions that are distributed by management elements that monitor and initiate actions ensuring workloads are deploying to the right destinations for your organisational needs. CLOUD ADOPTION DATA This look at a cloud adoption architecture data flow is not meant to be an all encompassing view of the exact flow. The idea is to give an architecture that you use to understand how elements and their data works through the entire cloud adoption architecture. With that in mind, the data flow shown is from the core data center on the left and works its way through the image repositories (images), automation orchestration (playbooks), and smart management (packages). From the image registries in each destination the data shows rolling out the workloads and server images onto the RHEL hosts.  In the cloud services, data flows show the gathering of insights and distribution of the automation action along with recommendations for the smart management to apply across the entire organisations architecture. This concludes the look at the cloud adoption architecture.  WHAT'S NEXT This was just a short overview of the example adoption architecture that makes up our architecture for the cloud adoption use case.  An overview of this series on cloud adoption portfolio architecture: 1. 2. 3. Catch up on any past articles you missed by following any published links above. This completes the series and we hope you enjoyed this architecture for cloud adoption. (Article co-authored by , Chief Architect Retail, Red Hat)&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/feWsgR4IXJI" height="1" width="1" alt=""/&gt;</content><dc:creator>Eric D. Schabell</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/schabell/jboss/~3/UQMoqclrSI0/cloud-adoption-example-adoption-architeccture.html</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 01 July 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/DEnNmKSq2WA/weekly-2021-07-01.html" /><category term="quarkus" /><category term="DMN" /><category term="Drools" /><category term="Wildfly" /><category term="Infinispan" /><category term="microservices" /><author><name>Jason Porter</name><uri>https://www.jboss.org/people/jason-porter</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-07-01.html</id><updated>2021-07-01T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, DMN, Drools, Wildfly, Infinispan, microservices"&gt; &lt;h1&gt;This Week in JBoss - 01 July 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Congrats to all the teams on their hard work!&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2021/06/29/infinispan-js-client"&gt;Inifinispan Node.js Client 0.9.0&lt;/a&gt; This was released last week and includes the ability for the Node.js client to connect with different SASL authentication mechanisms.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-1-1/"&gt;Eclipse Vert.x 4.1.1 released!&lt;/a&gt; Mostly bug fixes, but there were a few small features implemented. Check the announcement for more details.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-0-0-final-released/"&gt;Quarkus 2.0.0.Final released - Vert.x 4, MicroProfile 4, Continuous Testing and much more&lt;/a&gt;. Certainly a much anticipated release! Quarkus 2 has been in the works for months and the team is very excited for the release. A number of things have changed, so be sure to read the announcement and migration guide as you work on migrating those Quarkus apps!&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_from_the_community"&gt;From the community&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Karina Varela has a post about &lt;a href="https://developers.redhat.com/articles/2021/06/24/automating-rule-based-services-java-and-kogito"&gt;automating rule-based services with Java and Kogito&lt;/a&gt; up on the Red Hat Developer Blog. The post focuses on Kogito and using it to build, package, and automate the deployment of those rule-based services on Kubernetes and OpenShift!&lt;/p&gt; &lt;p&gt;James Falkner quickly tackles &lt;a href="https://developers.redhat.com/articles/2021/07/01/resteasy-reactive-and-more-quarkus-20"&gt;RESTEasy Reactive in Quarkus 2.0&lt;/a&gt; over on the Red Hat Developer Blog. With the release of Quarkus 2.0 there are a number of things that have improved. Using REST in a reactive way is one of them. Quarkus 2.0 utilizes Eclipse Vert.x 4 and does some optimizations at build time to further increase performance of REST services.&lt;/p&gt; &lt;p&gt;If you’re using Red Hat 3scale API Management and have a need to use WebSockets, be sure to check out Srikanth Kalluru’s &lt;a href="https://developers.redhat.com/articles/2021/07/01/how-expose-websocket-endpoint-using-red-hat-3scale-api-management"&gt;blog post&lt;/a&gt; for a quick three step walk through of how to get it done!&lt;/p&gt; &lt;p&gt;Mauro Vocale has a three part series, part four is not out yet, about moving Java programs to the cloud. &lt;a href="https://developers.redhat.com/articles/2021/06/25/making-java-programs-cloud-ready-part-1-incremental-approach-using-jakarta-ee"&gt;Part one&lt;/a&gt; introduces you to the legacy application and gets it up and running on OpenShift. This gets you familiar with what the application does and what to expect from further refactorings in subsequent posts. In &lt;a href="https://developers.redhat.com/articles/2021/06/28/making-java-programs-cloud-ready-part-2-upgrade-legacy-java-application-jakarta"&gt;part two&lt;/a&gt; the application will move from Java 8 to Java 11, JBoss EAP 7.3 and Jakarta EE. Lastly, in &lt;a href="https://developers.redhat.com/articles/2021/06/30/making-java-programs-cloud-ready-part-3-integrate-microprofile-services"&gt;part three&lt;/a&gt; the application will be moved to microservices. Additional monitoring tools will be introduced including Prometheus and Jaeger. The application will be updated for handling restarts, health checks, and other configuration settings.&lt;/p&gt; &lt;p&gt;Over in the BPM arena, there are three posts to take a look at: &lt;a href="https://blog.kie.org/2021/06/custom-logic-in-bpmn.html"&gt;Custom logic in BPMN&lt;/a&gt; by Kirill Gaevksii, &lt;a href="https://blog.kie.org/2021/07/how-to-start-contributing-to-drools-executable-model.html"&gt;How to start contributing to Drools executable model&lt;/a&gt; where Luca Molteni discusses getting your feet with contributing to Drools’s "Executable Model Compiler". Lastly, Matteo Mortari discusses &lt;a href="https://blog.kie.org/2021/06/intelligent-kafka-message-routing-using-drools-dmn-engine-and-apache-camel.html"&gt;using Drools DMN and Apache Camel to intelligently route Kafka messages&lt;/a&gt;. It includes a video as well as various examples and code to really sink your teeth into.&lt;/p&gt; &lt;p&gt;Wildfly 24.0.0.Final includes a preview of Jakarta EE 9.1 features. Jeff Mesnil discusses &lt;a href="https://www.wildfly.org//news/2021/07/01/wildfly-preview-bootable-jar/"&gt;how to try these features out using a bootable jar&lt;/a&gt;. There’s also a section about getting up and running using OpenShift!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_developers_on_film"&gt;Developers on film&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Get your popcorn ready and sit back to watch some videos from our community. Here are my top picks for this week’s editorial:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=JOv1K_wj6Vo"&gt;Quarkus Insights #54: Kotlin on Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=WyeaF2pk8Ec"&gt;Quarkus Insights #55: Quarkus 2.0 Launch&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=HQHjrf3i91Q"&gt;KIE Live #36: How to play with DMN&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=pj8or38w2eQ"&gt;KIE Live #37: How to work with dashboards layouts&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/jason-porter.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Jason Porter&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/DEnNmKSq2WA" height="1" width="1" alt=""/&gt;</content><dc:creator>Jason Porter</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-07-01.html</feedburner:origLink></entry><entry><title>Implementing Apache ActiveMQ-style broker meshes with Apache Artemis</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/2T4i68jSbSU/implementing-apache-activemq-style-broker-meshes-apache-artemis" /><author><name>Kevin Boone</name></author><id>2cfadff9-d418-49ea-a0f1-0c1aef7314a7</id><updated>2021-06-30T07:00:00Z</updated><published>2021-06-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://activemq.apache.org/"&gt;Apache ActiveMQ&lt;/a&gt; and &lt;a href="http://people.apache.org/~clebertsuconic/"&gt;Apache Artemis&lt;/a&gt; (or ActiveMQ Artemis) are open source message brokers with similar functionality. Both implementations are venerable, with histories that go back to the early 2000s. However, Artemis is in some senses a more modern implementation because, ironically, it has a smaller feature set. This makes Artemis easier to maintain, which is important if you're basing a commercial product on it. The smaller feature set means a smaller overall implementation, which fits well with developing &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Early versions of &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; were based on ActiveMQ, but attention has shifted to Artemis in &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_amq/7.1/html/introducing_red_hat_amq_7/index"&gt;AMQ 7&lt;/a&gt;. ActiveMQ is not maintained as vigorously as it once was by the open source community, but at the time of writing, Amazon is still offering a message broker service based on ActiveMQ. Whether it has a long-term future, at Amazon or elsewhere, remains to be seen.&lt;/p&gt; &lt;p&gt;Leaving aside ActiveMQ's complex niche features (such as message routing based on Apache Camel rules), ActiveMQ and Artemis look similar to the integrator and, in most practical applications, provide comparable throughput. However, they differ in important areas. Message distribution in the presence of multiple active brokers causes particular problems for integrators who want to move from ActiveMQ to Artemis.&lt;/p&gt; &lt;p&gt;This article describes subtleties that can lead to lost messages in an Artemis &lt;em&gt;active-active&lt;/em&gt; mesh. That architecture consists of multiple message brokers interconnected in a mesh, each broker with its own message storage, where all are simultaneously accepting messages from publishers and distributing them to subscribers. ActiveMQ and Artemis use different policies for message distribution. I will explain the differences and show a few ways to make Artemis work more like ActiveMQ in an active-active scenario.&lt;/p&gt; &lt;h2&gt;Configuring the Artemis broker mesh&lt;/h2&gt; &lt;p&gt;For simplicity, I'm assuming that the brokers in the mesh have network names like broker1, broker2, etc., and that each listens for all messaging protocols on port 61616 (this is the default for Artemis as well as ActiveMQ). The setup I describe below is for broker1, but there is a high degree of symmetry between the brokers, so it isn't hard to work out the other broker settings.&lt;/p&gt; &lt;p&gt;When creating a new broker, the usual approach is to run &lt;code&gt;artemis create brokerXXX&lt;/code&gt; to create an outline configuration. I'm assuming that you have done this initial configuration, and so only mesh-related configuration has to be added to &lt;code&gt;etc/broker.xml&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;The acceptor definition&lt;/h3&gt; &lt;p&gt;Every Artemis broker has at least one &lt;code&gt;acceptor&lt;/code&gt; definition that defines the TCP port and the protocols that will be accepted on that port. There's probably nothing different about this definition in a broker mesh, compared to a standalone broker. Here's an example, for a broker that accepts all wire protocols on port 61616:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;&lt;acceptor name="artemis"&gt;tcp://0.0.0.0:61616? protocols=CORE,AMQP,STOMP,HORNETQ,MQTT,OPENWIRE/&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In practice, an acceptor that handles multiple protocols will probably have a lot of additional configuration, but that's not really relevant here. In any case, the instance-creation step will already have created an outline entry. You'll need to change it only if you want a specific configuration, such as using different network interfaces for client and interbroker communication.&lt;/p&gt; &lt;h3&gt;The connectors&lt;/h3&gt; &lt;p&gt;Next, we need to define connectors. These are equivalent to the network connector definitions in ActiveMQ, but there is one significant difference: With Artemis, we usually define the broker itself as a connector. Here is an example:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; &lt;connectors&gt; &lt;connector name="myself"&gt;tcp://broker1:61616&lt;/connector&gt; &lt;connector name="broker2"&gt;tcp://broker2:61616&lt;/connector&gt; &lt;connector name="broker3"&gt;tcp://broker3:61616&lt;/connector&gt; &lt;/connectors&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The first entry, &lt;code&gt;myself&lt;/code&gt;, denotes the current broker with its hostname and port. Subsequent entries define the other brokers in the mesh. For symmetry, I could have given the self-referential connector the name &lt;code&gt;broker1&lt;/code&gt;, to match the other brokers that follow. This naming approach may be useful if you have a large mesh and you want to cut and paste your configuration from one broker to another. However, sometimes it is clearer to make the self-referential connector stand out in some way. In any case, the important point is to define connectors for every broker in the mesh, including this one.&lt;/p&gt; &lt;h3&gt;The broker mesh&lt;/h3&gt; &lt;p&gt;The final vital piece of configuration assembles the various broker connectors into a mesh. Artemis provides various discovery mechanisms by which brokers can find one another in the network. However, if you're more familiar with ActiveMQ, you're probably used to specifying the mesh members explicitly. The following example shows how to do that, for the connectors listed in the configuration just shown. Note that I'm referring to this broker itself as &lt;code&gt;myself&lt;/code&gt;, to match the previous connector definition. It would be a mistake to list the current broker as a cluster connection, which is why I prefer to use a distinctive name.&lt;/p&gt; &lt;pre&gt; &lt;code&gt; &lt;cluster-connections&gt; &lt;cluster-connection name="my_mesh"&gt; &lt;connector-ref&gt;myself&lt;/connector-ref&gt; &lt;message-load-balancing&gt;ON_DEMAND&lt;/message-load-balancing&gt; &lt;static-connectors&gt; &lt;connector-ref&gt;broker2&lt;/connector-ref&gt; &lt;connector-ref&gt;broker3&lt;/connector-ref&gt; &lt;/static-connectors&gt; &lt;/cluster-connection&gt; &lt;/cluster-connections&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: I'll have more to say about &lt;code&gt;message-load-balancing&lt;/code&gt; later.&lt;/p&gt; &lt;p&gt;You'll probably want to configure your clients to know about the mesh, as well. Again, Artemis provides a number of discovery mechanisms, allowing clients to determine the network topology without additional configuration. These don't work with all wire protocols (notably, there is no discovery mechanism for Advanced Message Queuing Protocol), and ActiveMQ users are probably familiar with configuring the client's connection targets explicitly. The usual mechanism is to list all the brokers in the mesh in the client's connection URI.&lt;/p&gt; &lt;h2&gt;Why the Artemis configuration isn't (yet) like ActiveMQ&lt;/h2&gt; &lt;p&gt;With the configuration in the previous section, you should have a working mesh. That is, you should be able to connect consumers to all the nodes, produce messages to any node, and have them routed to the appropriate consumer. However, this mesh won't behave exactly like ActiveMQ, because &lt;em&gt;Artemis mesh operation is not governed by client demand&lt;/em&gt;.&lt;/p&gt; &lt;h3&gt;Forwarding behavior&lt;/h3&gt; &lt;p&gt;In ActiveMQ, network connectors are described as "demand forwarding." This means that messages are accepted on a particular broker and remain there until a particular client requests them. If there are no clients for a particular queue, messages remain on the original broker until that situation changes.&lt;/p&gt; &lt;p&gt;On Artemis, forwarding behavior is controlled &lt;em&gt;by the brokers&lt;/em&gt;, and is only loosely associated with client load. In the previous section's configuration, I set &lt;code&gt;message-load-balancing=ON_DEMAND&lt;/code&gt;. This instructs the brokers not to forward messages for specific queues to brokers where there are, at present, no consumers for those queues. So if there are no consumers connected at all, the routing behavior is similar to that of ActiveMQ: Messages will accumulate on the broker that originally received them. If I had set &lt;code&gt;message-load-balancing=STRICT&lt;/code&gt;, the receiving broker would have divided the messages evenly between the brokers that defined that queue. With this configuration, the presence or absence of clients &lt;em&gt;should&lt;/em&gt; be irrelevant ... except it isn't quite that simple, and the complications are sometimes important.&lt;/p&gt; &lt;h3&gt;How the message queue is defined&lt;/h3&gt; &lt;p&gt;Even with &lt;code&gt;STRICT&lt;/code&gt; load balancing, brokers won't forward messages to other brokers that don't know about the queue. If queues are administratively defined, all brokers know about all queues and accept messages for them in &lt;code&gt;STRICT&lt;/code&gt; mode. If the queues are auto-created by clients, and there are no clients for a specific queue, a producer on broker1 could send a message for a queue that was not known on broker2. As a result, messages would never be forwarded. In short: &lt;em&gt;It makes a difference whether a queue is defined administratively or auto-created&lt;/em&gt;. There is no such difference in message distribution in ActiveMQ, because it is driven by client demand.&lt;/p&gt; &lt;p&gt;Even with &lt;code&gt;ON_DEMAND&lt;/code&gt; load balancing, Artemis's behavior is not the same as ActiveMQ's. A particular difference is that message distribution decisions are made when the message arrives. It is at that point that the broker sees what clients are connected and routes the message as it deems appropriate. If there are no clients for a specific queue &lt;em&gt;at that time&lt;/em&gt;, the message will not be routed.&lt;/p&gt; &lt;p&gt;What this means is that if a client that is connected to broker1 goes down for some reason, and then reconnects, it will not receive any of the messages that came in the meantime. Even if there are no other clients for that queue on any other broker, the message will not be routed from its original location. It's too late—the routing decision has already been made.&lt;/p&gt; &lt;p&gt;This is a particular problem for broker installations that are behind a load balancer or similar proxy. There's usually no way of knowing which broker a client will ultimately connect to because the load balancer will make that decision. But if a client has the bad fortune to get connected to a broker that has never hosted that client before, no messages that came earlier will be routed to it, even if it subscribes to a queue that has messages on some other broker. To fix this problem, we need &lt;em&gt;message redistribution&lt;/em&gt;.&lt;/p&gt; &lt;h2&gt;Message redistribution in Artemis&lt;/h2&gt; &lt;p&gt;ActiveMQ has no need for a message redistribution mechanism, because all message flows over the network connectors are coordinated by client demand. As we've seen, this is not the case for Artemis, where all message distribution is controlled by the brokers. In the usual run of events, distribution decisions are made when messages arrive, and they are irrevocable.&lt;/p&gt; &lt;p&gt;Artemis does have a way to redistribute messages after that point, but it is not enabled by default. The relevant setting is made on a specific address, or group of addresses, like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; &lt;address-setting match="#"&gt; &lt;redistribution-delay&gt;1000&lt;/redistribution-delay&gt; ... &lt;/address-setting&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The value supplied for &lt;code&gt;redistribution-delay&lt;/code&gt; is in units of milliseconds. This value is the length of time for which a broker will leave messages on a specific address that has no consumer, before sending them somewhere else. The default value is -1, meaning "do not redistribute.""&lt;/p&gt; &lt;p&gt;A redistribution delay of seconds or minutes, rather than milliseconds, probably creates less load on the broker and the network. In short, if you set an &lt;code&gt;ON_DEMAND&lt;/code&gt; load-balancing policy, and enable message redistribution with a relatively short delay, the broker mesh will largely look to clients like an ActiveMQ mesh.&lt;/p&gt; &lt;h2&gt;Why the Artemis configuration still isn't (exactly) like ActiveMQ's&lt;/h2&gt; &lt;p&gt;We have started to solve the problem of lost messages on Artemis. There are a number of subtle differences between Artemis and ActiveMQ, however, and it's impossible to get &lt;em&gt;exactly&lt;/em&gt; the same behavior that ActiveMQ implements.&lt;/p&gt; &lt;h3&gt;Message selectors&lt;/h3&gt; &lt;p&gt;A particular problem involves message selectors. If a client subscribes to a queue using a selector, it expects to receive only messages that match the selector. But what happens if different clients subscribe to the same queue, with different selectors, on different brokers? This is a rather specific problem, but it does come up. Artemis forwards messages according to whether there are consumers, not according to whether there are selectors. So there's every chance that messages will get forwarded to a broker whose consumers will not match the selector. These messages will never be consumed.&lt;/p&gt; &lt;p&gt;This isn't specifically an Artemis problem: Using selectors is somewhat problematic with a broker mesh, regardless of the implementation. Using selectors with a mesh isn't entirely robust on ActiveMQ, either: The broker has to maintain a "selector cache" to keep track of which selectors are active on which queues. Because it's impossible for the broker to know when clients come and go, the selector cache has to maintain tracking data for an extended period of time—perhaps indefinitely. This creates a memory burden, and as a result, there are different selector cache implementations available with different properties.&lt;/p&gt; &lt;p&gt;Artemis does not use selector caches, because it side-steps the issue of selector handling altogether. Unless your clients are configured to consume from all brokers concurrently (which isn't a bad idea in many applications), it's just not safe to use selectors.&lt;/p&gt; &lt;h3&gt;Message grouping&lt;/h3&gt; &lt;p&gt;There are a number of other broker features that don't work properly in a mesh, and don't work properly with ActiveMQ, either. The most troublesome is message grouping, which doesn't work at all in an Artemis mesh. It works partially with ActiveMQ, but isn't robust in the event of a client or broker outage. "Exclusive consumers" are also problematic on both brokers.&lt;/p&gt; &lt;p&gt;Recognizing the limitations described in this section, Red Hat is working on enhancements to Artemis that will allow brokers to re-route client connections to the brokers that are best placed to handle them. The work required is extensive because each of the various wire protocols that Artemis supports has its own way of dealing with dynamic load balancing.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In a broker mesh, Artemis uses a completely different strategy for message distribution from ActiveMQ. Understanding how Artemis works in this respect should go a long way to determining what changes need to be made to move from ActiveMQ to Artemis.&lt;/p&gt; &lt;p&gt;In particular, use the &lt;code&gt;ON_DEMAND&lt;/code&gt; load-balancing policy, and be sure to enable message redistribution. Some tuning may be needed to find the best redistribution delay for a particular application.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/30/implementing-apache-activemq-style-broker-meshes-apache-artemis" title="Implementing Apache ActiveMQ-style broker meshes with Apache Artemis"&gt;Implementing Apache ActiveMQ-style broker meshes with Apache Artemis&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/2T4i68jSbSU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Kevin Boone</dc:creator><dc:date>2021-06-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/30/implementing-apache-activemq-style-broker-meshes-apache-artemis</feedburner:origLink></entry><entry><title type="html">Quarkus 2.0.0.Final released - Vert.x 4, MicroProfile 4, Continuous Testing and much more</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Etd88sNTsro/" /><author><name /></author><id>https://quarkus.io/blog/quarkus-2-0-0-final-released/</id><updated>2021-06-30T00:00:00Z</updated><content type="html">The Quarkus team is proud to announce the availability of the Final release of Quarkus 2.0. This version has been a gigantic effort to bring Quarkus to a whole new level, while keeping its roots: fast boot, low memory usage and developer joy. A big thank you to everyone involved...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Etd88sNTsro" height="1" width="1" alt=""/&gt;</content><dc:creator /><feedburner:origLink>https://quarkus.io/blog/quarkus-2-0-0-final-released/</feedburner:origLink></entry><entry><title type="html">Infinispan Node.js client supports authentication</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VdaKbTVijgA/infinispan-js-client" /><author><name>Katia Aresti</name></author><id>https://infinispan.org/blog/2021/06/29/infinispan-js-client</id><updated>2021-06-29T12:00:00Z</updated><content type="html">NODE.JS CLIENT 0.9.0 Infinispan Node.js client version 0.9.0 was released last week with added support for different SASL authentication mechanisms. Up to now, our Node.js client could connect to Infinispan Server security realms with disabled authentication. DIGEST-MD5 Uses the MD5 hashing algorithm in addition to nonces to encrypt credentials. SCRAM Uses salt values in addition to hashing algorithms and nonce values to encrypt credentials. Hot Rod endpoints support SCRAM-SHA-1, SCRAM-SHA-256, SCRAM-SHA-384, SCRAM-SHA-512 hashing algorithms, in order of strength. EXTERNAL Uses client certificates to provide valid identities to Infinispan Server and enable encryption. OAUTHBEARER Uses tokens obtained via an OAuth 2.0 provider to securely connect to Infinispan Server. PLAIN: Sends credentials in plain text (unencrypted) over the wire in a way that is similar to HTTP BASIC authentication. Warning To secure Infinispan credentials, you should use PLAIN authentication only in combination with TLS encryption. RUN THE INFINISPAN SERVER Run the Infinispan Server with Docker or Podman docker run -it -p 11222:11222 -e USER="admin" -e PASS="password" quay.io/infinispan/server:12.1 podman run -it -p 11222:11222 -e USER="admin" -e PASS="password" --net=host quay.io/infinispan/server:12.1 Important If you are using Docker for Mac, there is a known limitation. You will need to and run the server manually. Run the Infinispan Server from the file system ./bin/cli.sh user create admin -p password ./bin/server.sh CREATE A CACHE FROM INFINISPAN CONSOLE Access the Infinispan Console in and create a text based cache, named it 'my-cache' with the provided configuration. Connect to Infinispan { "distributed-cache": { "mode": "SYNC", "encoding": { "media-type": "text/plain" }, "statistics": true } } USE THE NODE.JS CLIENT IN YOUR APPLICATION Add the dependency to your project. package.json "dependencies": { "infinispan": "^0.9.0" } Configure the Infinispan Node.js client to connect with authentication and then check the created cache entry from the console. application.js var connected = infinispan.client({port: 11222, host: '127.0.0.1'}, { cacheName: 'my-cache', authentication: { enabled: true, saslMechanism: 'DIGEST-MD5', userName: 'admin', password: 'password' } }); connected.then(function (client) { return client.put('key', 'value') .finally(function() { return client.disconnect(); }); }); TO GO FURTHER Full client documentation is now available in the . Jira tracker for this client is available .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VdaKbTVijgA" height="1" width="1" alt=""/&gt;</content><dc:creator>Katia Aresti</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/06/29/infinispan-js-client</feedburner:origLink></entry><entry><title type="html">Intelligent Kafka message routing using Drools DMN Engine and Apache Camel</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GCo0-uY8p0M/intelligent-kafka-message-routing-using-drools-dmn-engine-and-apache-camel.html" /><author><name>Matteo Mortari</name></author><id>https://blog.kie.org/2021/06/intelligent-kafka-message-routing-using-drools-dmn-engine-and-apache-camel.html</id><updated>2021-06-29T09:29:04Z</updated><content type="html">In this post I want to share an interesting use-case of Healthcare message routing, which we implemented using the and , in order to route and dispatch Patient’s Admission-Discharge-Transfer message types to the required and therefore queuing the message to the appropriate sub-system. I believe it is both a very pragmatic and interesting combination of technologies, while it also shows a blueprint and generalised pattern which can be easily replicated to other domains, in order to achieve intelligent message routing for Apache Kafka for many other use-cases. You can see the tech demo in action in this video and further below for additional details: The use-case for this tech demo is about specific types of message in the Healthcare domain, which contains information about the Patient’s Admission-Discharge-Transfer (ADT); these types of messages, depending on a set of rules, need to be dispatched to the appropriate Kafka topic(s). This way, further systems and applications downstream can connect to these queues and consume only the relevant messages of interest. EIP diagram for Intelligent Healthcare message routing Rules are formalised by the domain expert, in this case the Healthcare professional, who usually is not a software developer, but is knowledgeable and interested in the data contained in the message itself. This makes it a perfect fit for DMN, as the rules can be easily encoded using a DMN Decision Table, so that the decision about the destination queue names can easily be represented in a graphical format –one of the many key advantages of the DMN Standard. In the example screenshot, the domain expert is interested with the first rule in all ADT messages coming only from the application called “MMS”: these needs to be dispatched to the queue named “MMSAllADT” on Kafka. A second rule prescribe that ADT messages, again only from the application called “MMS”, but pertaining only to a Patient’s discharge (reported as “A03” on HL7), shall also be dispatched to topic “MMSDischarges” on Kafka. For the purpose of simplicity of the demo, the table contains only 2 criteria; naturally this table can be easily extended to accomodate all the specific rules required, as the demonstration video also shows. The Enterprise Integration Pattern diagram can be revisited, now annotated with the actual technologies used: For the purpose of this tech demo, the incoming message format is exchanged using the , and provided to a REST Endpoints, which may also serve as a . Naturally this could be extended to use more modern standards in the same healthcare domain, such as . It is important to reiterate this “Intelligent message routing” blueprint is very generic, therefore can be analogously architected on top of other business domains, such as , , etc. Another advantage of this blueprint is the separation of concerns between the business domain logic and the integration code; that is the separation between the rules formalised graphically by means of a DMN Decision Table to decide on which topic name the message shall be queued into, and the actual Apache Camel code implementing the integration between the different systems. This way, the business expert can focus on just and only the decision to be modelled using DMN, while the software developer can offer a very extensible and robust intelligent routing capability. Taking a little look under the hood, we highlight how the software developer using the Camel DSL only needs to write a minimal amount of code: I have highlighted visually where the Camel DSL integrates the Drools DMN Engine for the evaluation of the necessary business logic for the rules, in order to decide which Kafka topic(s) the message needs to be queued into, or eventually forwarded to a catch-all queue. This allow to maintain the set of rules and decisions using just and only the DMN model, separately from integration code. In the video, three scenarios are demonstrated: * Scenario 1: Normal routing This is the standard operating mode. * Scenario 2: Failure mode This is highlighting the catch-all queue for messages which are not matching any of the prescribed rules of the domain expert * Scenario 3: Add new rule One of the most important aspects we highlighted is the ability to update the rules for message dispatching, without touching a single line of code, and this scenario demonstrates how this aspect is technically fully under the control of the domain expert. Naturally, in a real world scenario, several actors and procedures are involved before rolling-over the update on the Production system, going beyond the merely technical requirements; the demo focuses only on the pragmatical aspects. CONCLUSIONS In this technical demo, we have seen how we can easily integrate and , to achieve Intelligent message routing on top of Apache Kafka. Specifically we have seen how to formalise the rules for the decision of the appropriate dispatch queue using DMN decision tables, so we can manage separately the business requirements from the actual integration code. What do you think of this use-case and this demo? Don’t hesitate to let us know in the comments below! The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GCo0-uY8p0M" height="1" width="1" alt=""/&gt;</content><dc:creator>Matteo Mortari</dc:creator><feedburner:origLink>https://blog.kie.org/2021/06/intelligent-kafka-message-routing-using-drools-dmn-engine-and-apache-camel.html</feedburner:origLink></entry><entry><title>Integrate Red Hat Data Grid and Red Hat JBoss Enterprise Application Platform on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9PywIHhcAuU/integrate-red-hat-data-grid-and-red-hat-jboss-enterprise-application-platform" /><author><name>Varsha Sharma</name></author><id>f8406fd2-6f02-490d-9d35-cbe35514e69f</id><updated>2021-06-29T07:00:00Z</updated><published>2021-06-29T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; can be used as an external cache container for application-specific data such as HTTP sessions in &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/application-platform"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP). This lets us scale the data layer independently from the application. It also makes it possible for JBoss EAP clusters residing in various domains to access data from the same Data Grid cluster.&lt;/p&gt; &lt;p&gt;This article offers quick instructions for getting Data Grid 8.1.1 working with JBoss EAP version 7.3.6 deployed on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;. For this integration, we will deploy Data Grid and JBoss EAP in the same project on OpenShift. We will &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.3/html-single/configuration_guide/index#jdg_externalize_http_sessions"&gt;use a Hot Rod store to externalize HTTP sessions&lt;/a&gt; to Data Grid.&lt;/p&gt; &lt;p&gt;I recently explored this integration for a Red Hat customer, and figured it would be helpful to put together detailed instructions for replicating it.&lt;/p&gt; &lt;h2&gt;Benefits of this integration&lt;/h2&gt; &lt;p&gt;This integration increases application scalability, elasticity, and session persistence. Offloading session data to a remote data grid makes the application tier more scalable and elastic, and it ensures the application will survive JBoss EAP node failures. A JVM failure will not cause you to lose session data.&lt;/p&gt; &lt;h2&gt;Setting up the environment&lt;/h2&gt; &lt;p&gt;The integration described in this article requires the following technologies:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;code&gt;oc&lt;/code&gt; client on a laptop&lt;/li&gt; &lt;li&gt;Red Hat JBoss Enterprise Application Platform 7.3.6&lt;/li&gt; &lt;li&gt;Data Grid 8.1.1&lt;/li&gt; &lt;li&gt;OpenShift 4.6.x&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Step 1: Set up Data Grid on OpenShift&lt;/h2&gt; &lt;p&gt;The first step in the process is to log in to your OpenShift cluster and create a new project. Once you've done that, you will install Data Grid 8.1.1 using the Data Grid Operator.&lt;/p&gt; &lt;h3&gt;Install the Data Grid Operator&lt;/h3&gt; &lt;p&gt;To install the Data Grid Operator, open your Red Hat OpenShift console, then navigate to &lt;strong&gt;Operators—&gt;Operator Hub—&gt;Data Grid 8.1.x&lt;/strong&gt;. For the approval strategy, select &lt;strong&gt;Automatic&lt;/strong&gt;. For the installation mode, select &lt;strong&gt;Namespace: &lt;project-name&gt;—&gt; Install&lt;/strong&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;NAME READY STATUS RESTARTS AGE infinispan-operator-88d585dd7-xc5xh 1/1 Running 0 58s&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: See &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.1/html/running_data_grid_on_openshift/installation#create_olm_subscription" target="_blank"&gt;&lt;em&gt;Installing Data Grid Operator on Red Hat OpenShift&lt;/em&gt;&lt;/a&gt; for further instructions.&lt;/p&gt; &lt;h3&gt;Create the Infinispan cluster&lt;/h3&gt; &lt;p&gt;Next, you will create an Infinispan custom resource (CR) for the cluster. Using the Data Grid Operator makes it easy to deploy Data Grid in a variety of configurations. Once you’ve deployed the operator from the OpenShift &lt;a href="https://docs.openshift.com/container-platform/4.5/operators/understanding/olm-understanding-operatorhub.html" target="_blank"&gt;OperatorHub&lt;/a&gt;, it exposes custom resources, called the Infinispan cluster and Infinispan cache, which Data Grid uses to provision the caches on an existing cluster.&lt;/p&gt; &lt;p&gt;Start by creating an Infinispan cluster from the OpenShift console by going to &lt;strong&gt;Installed Operators—&gt;Infinispan Cluster—&gt;Create&lt;/strong&gt;. Then, select the &lt;strong&gt;DataGrid&lt;/strong&gt; service type under the &lt;strong&gt;Service&lt;/strong&gt; field.&lt;/p&gt; &lt;h3&gt;Disable SSL security for HTTP communication&lt;/h3&gt; &lt;p&gt;You will need to disable the Secure Sockets Layer (SSL) communication between JBoss EAP and Data Grid and change the &lt;code&gt;endpointEncryption&lt;/code&gt; type to &lt;code&gt;none&lt;/code&gt; in the security spec:&lt;/p&gt; &lt;pre&gt; apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: eap-infinispan namespace: dgeap spec: service: type: DataGrid replicas: 1 security: endpointEncryption: type: None&lt;/pre&gt; &lt;h2&gt;Step 2: Deploy your application in JBoss EAP&lt;/h2&gt; &lt;p&gt;You can use one of two strategies to build and deploy the application in JBoss EAP:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use one of the JBoss EAP templates.&lt;/li&gt; &lt;li&gt;Use a binary build, the WAR artifact, and the official JBoss EAP container image.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We'll take the second approach for our implementation here.&lt;/p&gt; &lt;h3&gt;A note about JBoss EAP templates&lt;/h3&gt; &lt;p&gt;If you choose to deploy your application using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.3/html-single/getting_started_with_jboss_eap_for_openshift_container_platform/index#prepare_for_deployment"&gt;JBoss EAP templates&lt;/a&gt;, you will need to add the web-clustering Galleon layer to the Infinispan subsystem, as shown here:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; -p GALLEON_PROVISION_LAYERS=jaxrs-server, web-clustering \&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;GALLEON_PROVISION_LAYERS&lt;/code&gt; trims your server by removing pieces that will not be used. Note that the &lt;code&gt;jaxrs-server&lt;/code&gt; Galleon layer doesn't pull in the Infinispan subsystem; for that, you will also need the web-clustering Galleon layer. See the documentation for &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.3/html-single/getting_started_with_jboss_eap_for_openshift_online/index#available-jboss-eap-layers_default"&gt;available JBoss EAP layers&lt;/a&gt; for more information.&lt;/p&gt; &lt;h3&gt;Deployment with the JBoss EAP container image&lt;/h3&gt; &lt;p&gt;First, you will import the JBoss EAP container image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc import-image jboss-eap-7/eap73-openjdk11-openshift-rhel8 --from=registry.redhat.io/jboss-eap-7/eap73-openjdk11-openshift-rhel8 --confirm&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Deploy the WAR file&lt;/h4&gt; &lt;p&gt;Deploying an external WAR file on OpenShift is a two-step process. First, you will define and create the new application without providing a source of any type. To ensure that no source is provided to the &lt;code&gt;oc&lt;/code&gt; client, create an empty directory and use it to set up the new application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mkdir /tmp/nocontent $ oc describe is jboss-eap70-openshift -n openshift $ oc new-app jboss-eap70-openshift:&lt;tag&gt;~/tmp/nocontent --name=verify-cluster(APPLICATION_NAME)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This results in a new build being created, which has the same name as the application. You can start the build by providing the binary source:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc start-build APPLICATION_NAME --from-file=/tmp/verify-cluster.war (application under /tmp)&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can download the application source from my &lt;a href="https://github.com/varsharain-a11y/distributable"&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;h4&gt;Mount a config map to execute the custom scripts&lt;/h4&gt; &lt;p&gt;To include custom scripts when starting JBoss EAP from an image, we have to mount the config map to be executed as &lt;code&gt;postconfigure.sh&lt;/code&gt;. To start, create two files, one named &lt;code&gt;actions.cli&lt;/code&gt; and one named &lt;code&gt;postconfigure.sh&lt;/code&gt;. Here is the content for &lt;code&gt;actions.cli&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;embed-server --std-out=echo --admin-only --server-config=standalone-openshift.xml /socket-binding-group=standard-sockets/remote-destination-outbound-socket-binding=remote-rhdg-server:add(host=eap-infinispan.dgeap.svc.cluster.local,port=11222) batch /subsystem=infinispan/remote-cache-container=rhdg:add(default-remote-cluster=data-grid-cluster,\ properties={infinispan.client.hotrod.use_auth=true,infinispan.client.hotrod.sasl_properties.javax.security.sasl.qop=auth,infinispan.client.hotrod.sasl_mechanism=SCRAM-SHA-512,infinispan.client.hotrod.auth_username=developer,infinispan.client.hotrod.auth_server_name=eap-infinispan,infinispan.client.hotrod.auth_password=Cx2ak@E9fGcaSfs4,infinispan.client.hotrod.use_ssl=false,infinispan.client.hotrod.auth_realm=default}) /subsystem=infinispan/remote-cache-container=rhdg/remote-cluster=data-grid-cluster:add(socket-bindings=[remote-rhdg-server]) run-batch batch /subsystem=infinispan/cache-container=web/invalidation-cache=infinispan:add() /subsystem=infinispan/cache-container=web/invalidation-cache=infinispan/store=hotrod:add( remote-cache-container=rhdg,\ fetch-state=false,\ purge=false,\ passivation=false,\ shared=true) /subsystem=infinispan/cache-container=web/invalidation-cache=infinispan/component=transaction:add(mode=BATCH) /subsystem=infinispan/cache-container=web/invalidation-cache=infinispan/component=locking:add(isolation=REPEATABLE_READ) /subsystem=infinispan/cache-container=web:write-attribute(name=default-cache,value=infinispan) run-batch stop-embedded-server &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And here is the content for postconfigure.sh:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$JBOSS_HOME/bin/jboss-cli.sh --file=$JBOSS_HOME/extensions/actions.cli&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, you'll mount the config map:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create a config map with the content (&lt;code&gt;actions.cli&lt;/code&gt;) that we need to include in the &lt;code&gt;postconfigure.sh&lt;/code&gt;: &lt;pre&gt; oc create configmap jboss-cli --from-file=postconfigure.sh=postconfigure.sh --from-file=actions.cli=actions.cli&lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Mount the config map into the pods via the deployment controller or deployment: &lt;pre&gt; oc set volume deployment/verify-cluster --add --name=jboss-cli -m /opt/eap/extensions -t configmap --configmap-name=jboss-cli --default-mode='0777' &lt;/pre&gt; &lt;/li&gt; &lt;li&gt;Check the pod status: &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get po NAME READY STATUS RESTARTS AGE eap-infinispan-0 1/1 Running 1 13d infinispan-operator-59949fc49c-vk22x 1/1 Running 0 20d verify-cluster-1-build 0/1 Completed 0 20d verify-cluster-2-build 0/1 Completed 0 20d verify-cluster-69bc4ff545-7vkb5 1/1 Running 0 6d6h&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Update the ports section of the deployment&lt;/h4&gt; &lt;p&gt;Enter the following to update the &lt;code&gt;ports&lt;/code&gt; section of the deployment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$oc edit deployment/verify-cluster ports: - containerPort: 8080 name: http protocol: TCP - containerPort: 8443 name: https protocol: TCP - containerPort: 8778 name: jolokia protocol: TCP&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;Expose the service needed for JBoss EAP&lt;/h4&gt; &lt;p&gt;Enter the following to expose the service needed for JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc expose svc/verify-cluster (name of servicefrom [ $ oc get svc]) $ oc get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE verify-cluster ClusterIP 172.30.102.7 &lt;none&gt; 8080/TCP,8443/TCP,8778/TCP 20d $ oc get routes NAME HOST/PORT PATH SERVICES PORT TERMINATION WILDCARD verify-cluster verify-cluster-ssldgeap.apps.test1234.lab.rdu2.cee.redhat.com verify-cluster 8080-tcp None&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Copy the URL from the output (&lt;code&gt;verify-cluster-ssldgeap.apps.test1234.lab.rdu2.cee.redhat.com&lt;/code&gt;) to a web browser and append the context path name (&lt;code&gt;/verify-cluster&lt;em&gt;name of context path&lt;/em&gt;&lt;/code&gt;) to access the application, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/eap_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/eap_1.png?itok=IzAX7Qbh" width="600" height="599" alt="A screenshot of the deployed application accessed via a route." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The deployed application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Step 3: Test the HTTP session externalization&lt;/h2&gt; &lt;p&gt;On incrementing the counter while accessing the application, observe the logs:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;07:50:55,023 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Session:iMePP8CA-5jsQM5XnOiXBI03u-mDeUmMpKmyj71a, IP :verify-cluster-69bc4ff545-7vkb5/10.128.3.61, Hostname: verify-cluster-69bc4ff545-7vkb5, Visit Count:1 07:51:01,904 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Increment visitCount to 2 07:51:01,905 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Session:iMePP8CA-5jsQM5XnOiXBI03u-mDeUmMpKmyj71a, IP :verify-cluster-69bc4ff545-7vkb5/10.128.3.61, Hostname: verify-cluster-69bc4ff545-7vkb5, Visit Count:2 07:51:03,907 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Increment visitCount to 3 07:51:03,907 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Session:iMePP8CA-5jsQM5XnOiXBI03u-mDeUmMpKmyj71a, IP :verify-cluster-69bc4ff545-7vkb5/10.128.3.61, Hostname: verify-cluster-69bc4ff545-7vkb5, Visit Count:3 07:51:05,056 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Increment visitCount to 4 07:51:05,056 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Session:iMePP8CA-5jsQM5XnOiXBI03u-mDeUmMpKmyj71a, IP :verify-cluster-69bc4ff545-7vkb5/10.128.3.61, Hostname: verify-cluster-69bc4ff545-7vkb5, Visit Count:4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Scale down the JBoss EAP pod to 0 and then try scaling it up. The counter should not be reset:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;09:44:40,628 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management interface listening on http://0.0.0.0:9990/management 09:44:40,628 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0054: Admin console is not enabled 09:44:46,960 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Increment visitCount to 5 09:44:46,962 INFO [com.example.verify_cluster.CounterServlet] (default task-1) Session:q3QKwxM0GPvrSssbE6ubuR9866iNIn9dEuJrQuS1, IP :verify-cluster-69bc4ff545-dgkls/10.128.3.26, Hostname: verify-cluster-69bc4ff545-dgkls, Visit Count:5&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, log in to the Data Grid pod to verify whether a cache was created. A local cache is automatically created with a web application name such as &lt;code&gt;verify-cluster.war&lt;/code&gt;. Use the CLI to connect to the remote Data Grid server to verify the cluster:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc rsh eap-infinispan-0 [eap-infinispan-0-56270@infinispan//containers/default]&gt; ls caches ___protobuf_metadata ___script_cache Verify-cluster.war&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 4: Enable security between JBoss EAP and Data Grid&lt;/h2&gt; &lt;p&gt;To externalize a session to Data Grid with security enabled, add the following Hot Rod client properties to your config map:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;infinispan.client.hotrod.use_ssl=true, infinispan.client.hotrod.auth_realm=default,infinispan.client.hotrod.trust_store_path=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On the Data Grid end, create an infinispan cluster using Endpoint Encryption:&lt;/p&gt; &lt;pre&gt; spec: ... security: endpointEncryption: certSecretName: eap-infinispan-cert-secret certServiceName: &lt;a href="http://service.beta.openshift.io/"&gt;service.beta.openshift.io&lt;/a&gt; type: Service&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;You have successfully externalized the session data!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/06/29/integrate-red-hat-data-grid-and-red-hat-jboss-enterprise-application-platform" title="Integrate Red Hat Data Grid and Red Hat JBoss Enterprise Application Platform on Red Hat OpenShift"&gt;Integrate Red Hat Data Grid and Red Hat JBoss Enterprise Application Platform on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9PywIHhcAuU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Varsha Sharma</dc:creator><dc:date>2021-06-29T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/06/29/integrate-red-hat-data-grid-and-red-hat-jboss-enterprise-application-platform</feedburner:origLink></entry><entry><title type="html">Make SAP Cloud Native and Event Driven in 4 days</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/dQ5fPAMsie4/make-sap-cloud-native-and-event-driven.html" /><author><name>CHRISTINA の J老闆</name></author><id>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/SMs4oxqS8uU/make-sap-cloud-native-and-event-driven.html</id><updated>2021-06-28T11:00:00Z</updated><content type="html">Recently I had an opportunity to work with Sanket Taur (IBM UK) and his team on a demo, showcasing how Red Hat products can help speed up innovation with SAP Landscapes. To be honest I was shocked at how little time we were given to create the entire demo from scratch. It’s less than a week. While still doing our day job, having a couple of hours per day to work on it. If this doesn’t convince you..  I don’t know any other stronger proof than this, to show how agile and fast a cloud solution can be from development to production.  I encourage you to attend Sanket’s session for more details, this blog is JUST my view on the demo, and things I did to make it running. The demo was a simple approval process of Sales Orders. The SOs are created in the Core SAP platform (In this case ES5), therefore we need to create an application that speaks to the Core SAP platform and retrieve all the data needed.  First thing first, we need a Kubernetes(k8s) platform. And then I used Camel K — an enhanced framework based on Camel (part of Red Hat Integration product) to create the application. There was some mixup during the setup, instead of the OData v4 endpoint from ES5 for SO, line items and customer details. I was given an OData v2 endpoint. (Needless to say, how more efficient the OData v4 is, compared to v2. Please do update it when you have a chance). Note that Camel K only supports OData v4. HOWEVER, we can still get the results using normal REST API calls (So you are still covered).   This is how Camel helps you retrieve all the information needed. As you can see I have made several requests to get all the data needed as well as doing some transformation to extract results to return.    from("direct:getSO")    .setHeader("Authorization").constant("Basic XXXX")    .setHeader("Accept").constant("application/json")    .toD("https://sapes5.sapdevcenter.com/sap/opu/odata/iwbep/GWSAMPLE_BASIC/SalesOrderSet('${header.SalesOrderID}')?bridgeEndpoint=true")    .unmarshal().json()    .setHeader("CustomerID").simple("${body[d][CustomerID]}")    .marshal().json()    .bean(this, "setSO(\"${body}\",\"${headers.CustomerID}\")") ;  from("direct:getItems")      .setHeader("Authorization").constant("Basic XXXX")      .setHeader("Accept").constant("application/json") .toD("https://sapes5.sapdevcenter.com/sap/opu/odata/iwbep/GWSAMPLE_BASIC/SalesOrderSet('${header.SalesOrderID}')/ToLineItems?bridgeEndpoint=true")      .unmarshal().json()      .marshal().json()      .bean(this, "setPO(\"${body}\")") ;  from("direct:getCustomer")      .setHeader("Authorization").constant("Basic XXXX")      .setHeader("Accept").constant("application/json") .toD("https://sapes5.sapdevcenter.com/sap/opu/odata/iwbep/GWSAMPLE_BASIC/BusinessPartnerSet('${header.CustomerID}')?bridgeEndpoint=true") .unmarshal().json() .marshal().json()     .bean(this, "setCust(\"${body}\")") ;   The endpoints to trigger the call to SAP, is exposed as an API. Here I use Apicurio Studio to define the API contract. With two endpoints, fetch and fetchall. One returns SO, PO and Customer data, where the other one returns a collection of them.    We can now export the definition as a OpenAPI Specification contract in the form of YAML ( to see the yaml). Save the file into the folder of where your Camel application is. Add the API yaml file name to your Camel K application mode line, and Camel K will automatically map your code to this contract. // camel-k: language=java dependency=camel-openapi-java open-api=ibm-sap.yaml dependency=camel-jackson   By using the Camel K CLI tool. Run the command to deploy the code to the OpenShift platform.  kamel run SapOdata.java   And you should now see a microservice running. Did you notice how Camel K helps you, not only it detects and loads the libraries needed for you, but also containerised it as a running instance.  Go to my  to see the full code and running instructions.   Kafka was used in the middle to set the event driven architecture. So the SO approval application can notify the shopping cart client when it’s been approved.  Since everything was put together in a week, with everyone in different timezones, miss communication will happen. What I did not realize was that all the client applications, SO approval and shopping carts were all written in JavaScript, and must communicate via HTTP. But Kafka only does Kafka protocols!!! Therefore, I set up an Http Bridge in front of the Kafka clusters, so it will now translate the Kafka protocols.  And now clients can access the topic via HTTP endpoints.  For more information on how to set, go to my  for more detailed instructions.   Last but not least, we need to migrate all UI5 SAP web applications to OpenShift. The UI5 is basically an NODEJS app. We first create the docker file to containerize it. And push it to a container registry. docker push quay.io/&lt;YOUR_REPO&gt;/socreate   And deploy the application to OpenShift. oc new-app quay.io/&lt;YOUR_REPO&gt;/socreate:latest --as-deployment-config   BUT WAIT!! Since UI5 only does binds to *localhost* (weird..), we need to add a proxy that can tunnel traffic to it. Therefore, I added a sidecar proxy running right next to the NodeJS application. By adding the following configuration.  spec:      containers:        - name: nginx          image: quay.io/weimei79/nginx-sidecar          ports:            - containerPort: 8081              protocol: TCP          resources:            limits:              cpu: 500m              memory: 1Gi          terminationMessagePath: /dev/termination-log          terminationMessagePolicy: File           imagePullPolicy: Always   This will start the proxy, and since this NGINX proxy starts on port 8081, make sure you update all related settings on OpenShift. oc expose dc socreate --port=8181   oc expose svc socreate   And this is how you would migrate the UI5 application from a local SAP instance onto OpenShift. More detailed migration instructions, check out my .    Once it’s done, you can see all the applications are running as a container on the cloud. And ready to approve the SOs.    This is actual developer view on top of our demo OpenShift platform   Thank you Sanket for this fun ride, all the nail biting moments, but this is all the fun in IT right? We work through problems, tackle issues and ultimately get everything done! &#x1f642; If you are a SAPer, and want to join the container world of cloud, what are you still waiting for? Join the ride! This is the story on how we made SAP Cloud Native and Event Driven in 4 days.  To see the full version, be sure to attend Sanket’s session: SAP &amp;amp; OpenShift: From classic ABAP development to cloud native applications: Use cases and reference architecture to implement with SAP Landscapes to unlock innovation enabled by Hybrid Cloud and Red Hat OpenShift. Register here:&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/dQ5fPAMsie4" height="1" width="1" alt=""/&gt;</content><dc:creator>CHRISTINA の J老闆</dc:creator><feedburner:origLink>http://feedproxy.google.com/~r/blogspot/hFXzh/~3/SMs4oxqS8uU/make-sap-cloud-native-and-event-driven.html</feedburner:origLink></entry></feed>
